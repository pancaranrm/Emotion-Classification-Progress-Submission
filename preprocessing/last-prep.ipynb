{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61ada006",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pemrosesan ulang selesai. Data siap digunakan untuk model!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import resample\n",
    "\n",
    "# --- LANGKAH 1: LOAD DATA ---\n",
    "df = pd.read_csv('../dataset/merged.csv')\n",
    "ks = pd.read_csv('../dataset/kamus_slang.csv')\n",
    "\n",
    "# --- LANGKAH 2: MEMBANGUN KAMUS SLANG & STOPWORDS ---\n",
    "# Mapping Slang\n",
    "slang_dict = {}\n",
    "for _, row in ks.dropna(subset=['slang', 'formal']).iterrows():\n",
    "    slang_dict[str(row['slang']).strip().lower()] = str(row['formal']).strip().lower()\n",
    "\n",
    "# Kamus Kata Non-Emosi (Hasil analisis frekuensi sebelumnya)\n",
    "stop_umum = ['yang', 'dan', 'di', 'ini', 'itu', 'dari', 'untuk', 'dengan', 'ada', 'adalah', 'pada', 'sebagai', 'akan', 'sudah', 'bisa', 'saat', 'dalam', 'ke', 'oleh', 'bagi', 'serta', 'bahwa', 'maka', 'namun', 'tersebut']\n",
    "stop_konteks = ['indonesia', 'pendidikan', 'ai', 'teknologi', 'digital', 'guru', 'sekolah', 'negara', 'ekonomi', 'politik', 'pemerintah', 'rakyat', 'startup', 'program', 'sistem', 'data', 'manusia', 'dunia', 'prabowo']\n",
    "stop_sosmed = ['ya', 'buat', 'saja', 'juga', 'kalau', 'memang', 'banyak', 'lagi', 'bikin', 'pakai', 'punya', 'deh', 'sih', 'kok', 'amp', 'the', 'and', 'to', 'in', 'of', 'a']\n",
    "custom_stopwords = set(stop_umum + stop_konteks + stop_sosmed)\n",
    "\n",
    "# --- LANGKAH 3: FUNGSI PREPROCESSING TERINTEGRASI ---\n",
    "def final_preprocess(text):\n",
    "    if not isinstance(text, str): return \"\"\n",
    "    # a. Cleaning: URL, Mention, Hashtag, Non-huruf\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+|@\\w+|#\\w+', '', text)\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', ' ', text)\n",
    "    # b. Case Folding\n",
    "    text = text.lower()\n",
    "    # c. Slang Mapping & Stopword Removal\n",
    "    words = text.split()\n",
    "    clean_words = []\n",
    "    for w in words:\n",
    "        w_formal = slang_dict.get(w, w) # Ubah jadi baku\n",
    "        if w_formal not in custom_stopwords: # Filter jika bukan kata emosi\n",
    "            clean_words.append(w_formal)\n",
    "    return \" \".join(clean_words).strip()\n",
    "\n",
    "# Eksekusi Preprocessing\n",
    "df['caption_cleaned'] = df['caption'].apply(final_preprocess)\n",
    "\n",
    "# Hapus data yang kosong setelah dibersihkan\n",
    "df = df.dropna(subset=['caption_cleaned'])\n",
    "df = df[df['caption_cleaned'] != '']\n",
    "\n",
    "# --- LANGKAH 4: SPLITTING (70/15/15) ---\n",
    "train_df, temp_df = train_test_split(df, test_size=0.3, random_state=42, stratify=df['emotion'])\n",
    "val_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42, stratify=temp_df['emotion'])\n",
    "\n",
    "# --- LANGKAH 5: NATURAL QUASI-BALANCING (Hanya Train) ---\n",
    "max_c = train_df['emotion'].value_counts().max()\n",
    "quasi_list = []\n",
    "for emo in train_df['emotion'].unique():\n",
    "    d = train_df[train_df['emotion'] == emo]\n",
    "    if len(d) == max_c:\n",
    "        quasi_list.append(d)\n",
    "    else:\n",
    "        # Target: Original + 75% Gap + Random Jitter\n",
    "        target = int(len(d) + (max_c - len(d)) * 0.75) + random.randint(-15, 15)\n",
    "        quasi_list.append(resample(d, replace=True, n_samples=target, random_state=42))\n",
    "\n",
    "train_final = pd.concat(quasi_list)\n",
    "\n",
    "# --- LANGKAH 6: SIMPAN DATA ---\n",
    "train_final.to_csv('train_final.csv', index=False)\n",
    "val_df.drop_duplicates(subset=['caption_cleaned']).to_csv('val_final.csv', index=False)\n",
    "test_df.drop_duplicates(subset=['caption_cleaned']).to_csv('test_final.csv', index=False)\n",
    "\n",
    "print(\"Pemrosesan ulang selesai. Data siap digunakan untuk model!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebc01321",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c225390d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
