{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "675acc9e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8e588cf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sedang melakukan preprocessing...\n",
      "--- Hasil Akhir ---\n",
      "Jumlah Data Training (Quasi): 7826\n",
      "Jumlah Data Validation (Orig): 763\n",
      "Jumlah Data Testing (Orig)   : 764\n",
      "\n",
      "Distribusi Emosi Training:\n",
      "emotion\n",
      "Anticipation    1162\n",
      "Trust           1010\n",
      "Fear             972\n",
      "Sadness          954\n",
      "Anger            942\n",
      "Joy              933\n",
      "Surprise         929\n",
      "Disgust          924\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import resample\n",
    "\n",
    "# 1. LOAD DATA\n",
    "df = pd.read_csv('../dataset/merged.csv')\n",
    "ks = pd.read_csv('../dataset/kamus_slang.csv')\n",
    "\n",
    "# 2. PROSES KAMUS SLANG (Membangun Mapping)\n",
    "slang_dict = {}\n",
    "# Mengambil dari kolom slang & formal\n",
    "for _, row in ks.dropna(subset=['slang', 'formal']).iterrows():\n",
    "    slang_dict[str(row['slang']).strip().lower()] = str(row['formal']).strip().lower()\n",
    "\n",
    "# Mengambil data dari header dan kolom ketiga (format: \"slang;formal\")\n",
    "header_pair = 'aamiin;amin'.split(';')\n",
    "slang_dict[header_pair[0].strip().lower()] = header_pair[1].strip().lower()\n",
    "\n",
    "for val in ks[ks.columns[2]].dropna():\n",
    "    if ';' in str(val):\n",
    "        parts = str(val).split(';')\n",
    "        if len(parts) >= 2:\n",
    "            slang_dict[parts[0].strip().lower()] = parts[1].strip().lower()\n",
    "\n",
    "# 3. FUNGSI PREPROCESSING DETAIL\n",
    "def detail_preprocess(text, mapping):\n",
    "    if not isinstance(text, str): return \"\"\n",
    "    \n",
    "    # a. Hapus URL (http, https, www)\n",
    "    text = re.sub(r'http\\S+|www\\S+|https\\S+', '', text)\n",
    "    \n",
    "    # b. Hapus Mention (@user) dan Hashtag (#tag)\n",
    "    text = re.sub(r'@\\w+|#\\w+', '', text)\n",
    "    \n",
    "    # c. Hapus karakter non-alfabet (Simbol, Angka, Emoticon jika ingin dibersihkan)\n",
    "    # Catatan: Jika ingin mempertahankan emosi dari tanda seru/tanya, hapus baris ini\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', ' ', text)\n",
    "    \n",
    "    # d. Case Folding (Kecilkan semua huruf)\n",
    "    text = text.lower()\n",
    "    \n",
    "    # e. Normalisasi Karakter Berulang (misal: \"Horeeee\" -> \"Horee\")\n",
    "    text = re.sub(r'(.)\\1+', r'\\1\\1', text)\n",
    "    \n",
    "    # f. Normalisasi Kamus Alay / Slang\n",
    "    words = text.split()\n",
    "    normalized_words = [mapping.get(w, w) for w in words]\n",
    "    \n",
    "    return \" \".join(normalized_words).strip()\n",
    "\n",
    "# Terapkan Preprocessing Detail\n",
    "print(\"Sedang melakukan preprocessing...\")\n",
    "df['caption_cleaned'] = df['caption'].apply(lambda x: detail_preprocess(x, slang_dict))\n",
    "\n",
    "# 4. DATA SPLITTING (70/15/15) - Lakukan sebelum balancing\n",
    "train_df, temp_df = train_test_split(df, test_size=0.3, random_state=42, stratify=df['emotion'])\n",
    "val_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42, stratify=temp_df['emotion'])\n",
    "\n",
    "# 5. NATURAL QUASI-BALANCING (Hanya untuk Data Training)\n",
    "max_count = train_df['emotion'].value_counts().max() \n",
    "quasi_list = []\n",
    "\n",
    "for emotion in train_df['emotion'].unique():\n",
    "    df_emo = train_df[train_df['emotion'] == emotion]\n",
    "    current_count = len(df_emo)\n",
    "    \n",
    "    if current_count == max_count:\n",
    "        quasi_list.append(df_emo)\n",
    "    else:\n",
    "        # Target: Original + 75% Gap + Random Jitter agar natural\n",
    "        target = int(current_count + (max_count - current_count) * 0.75)\n",
    "        target += random.randint(-25, 25) \n",
    "        \n",
    "        df_resampled = resample(df_emo, replace=True, n_samples=target, random_state=42)\n",
    "        quasi_list.append(df_resampled)\n",
    "\n",
    "train_natural = pd.concat(quasi_list)\n",
    "\n",
    "# 6. SIMPAN HASIL FINAL\n",
    "train_natural.to_csv('../train_natural_quasi.csv', index=False)\n",
    "val_df.to_csv('../val_final.csv', index=False)\n",
    "test_df.to_csv('../test_final.csv', index=False)\n",
    "\n",
    "print(\"--- Hasil Akhir ---\")\n",
    "print(f\"Jumlah Data Training (Quasi): {len(train_natural)}\")\n",
    "print(f\"Jumlah Data Validation (Orig): {len(val_df)}\")\n",
    "print(f\"Jumlah Data Testing (Orig)   : {len(test_df)}\")\n",
    "print(\"\\nDistribusi Emosi Training:\")\n",
    "print(train_natural['emotion'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "415051c7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
